{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\Abdullah Al Amin\\.dgl/ACM.mat from https://data.dgl.ai/dataset/ACM.mat...\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import scipy.io as sio\n",
    "import urllib.request\n",
    "\n",
    "from dgl.data.utils import download, get_download_dir, _get_dgl_url\n",
    "\n",
    "\n",
    "url = 'dataset/ACM.mat'\n",
    "data_path = get_download_dir() + '/ACM.mat'\n",
    "download(_get_dgl_url(url), path=data_path)\n",
    "\n",
    "data = sio.loadmat(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "metapaths = [['written-by','writing'],['is-about','has']]\n",
    "def get_acm_data():\n",
    "    \n",
    "    G = dgl.heterograph({\n",
    "        ('paper', 'written-by', 'author') : data['PvsA'].nonzero(),\n",
    "        ('author', 'writing', 'paper') : data['PvsA'].transpose().nonzero(),\n",
    "        ('paper', 'citing', 'paper') : data['PvsP'].nonzero(),\n",
    "        ('paper', 'cited', 'paper') : data['PvsP'].transpose().nonzero(),\n",
    "        ('paper', 'is-about', 'subject') : data['PvsL'].nonzero(),\n",
    "        ('subject', 'has', 'paper') : data['PvsL'].transpose().nonzero(),\n",
    "        ('paper', 'published-in', 'venue') : data['PvsV'].nonzero(),\n",
    "        ('venue', 'published', 'paper') : data['PvsV'].transpose().nonzero(),\n",
    "        ('paper', 'related-to', 'field') : data['PvsL'].nonzero(),\n",
    "        ('field', 'described-by', 'paper') : data['PvsL'].transpose().nonzero(),\n",
    "        ('paper', 'contains', 'term'):  data['PvsT'].nonzero(),\n",
    "        ('term', 'consist-of', 'paper'):  data['PvsT'].transpose().nonzero()\n",
    "    })\n",
    "    \n",
    "    pvc = data['PvsC'].tocsr()\n",
    "    c_selected = [0, 11, 13]  # SODA, COLT, VLDB\n",
    "    p_selected = pvc[:, c_selected].tocoo()\n",
    "\n",
    "    labels = pvc.indices\n",
    "    labels[labels == 11] = 1\n",
    "    labels[labels == 13] = 2\n",
    "    labels = torch.tensor(labels).long()\n",
    "    \n",
    "    p_vs_c_filter = pvc[:, c_selected]\n",
    "    p_sel = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "    p_vs_t = data['PvsT'].tocsr()[p_sel]\n",
    "    features = torch.FloatTensor(data['PvsT'].toarray())\n",
    "\n",
    "\n",
    "    num_classes = len(c_selected)\n",
    "    # split of train/val/test \n",
    "    pid = p_selected.row\n",
    "    shuffle = np.random.permutation(pid)\n",
    "    train_idx = torch.tensor(shuffle[0:1200]).long()\n",
    "    val_idx = torch.tensor(shuffle[1200:1500]).long()\n",
    "    test_idx = torch.tensor(shuffle[1500:]).long()\n",
    "    \n",
    "    return G, labels, train_idx, val_idx, test_idx,num_classes, features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousRGCNLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, etypes):\n",
    "        super(HeterogeneousRGCNLayer, self).__init__()\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(input_size, output_size) for name in etypes\n",
    "            })\n",
    "        \n",
    "        \n",
    "    def forward(self, G, feat_dict):\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousRGCN(nn.Module):\n",
    "    def __init__(self, G, input_size, hidden_layer_size, output_size):\n",
    "        super(HeterogeneousRGCN, self).__init__()\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), input_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        self.layer1 = HeterogeneousRGCNLayer(input_size, hidden_layer_size, G.etypes)\n",
    "        self.layer2 = HeterogeneousRGCNLayer(hidden_layer_size, output_size, G.etypes)\n",
    "        \n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        return h_dict['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_accuracy_csv(model,loss, train_acc, validation_acc, test_acc, total_epoch, total_duration, timestamp):\n",
    "    accuracy_csv_data = []\n",
    "    accuracy_csv_data.append([model,str(loss),str(train_acc),str(validation_acc), str(test_acc), str(total_epoch), str(total_duration), str(timestamp)])\n",
    "    return accuracy_csv_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file_name, columns_name, data):\n",
    "    df_csv = pd.DataFrame(data, columns = columns_name)\n",
    "    if not os.path.isfile(file_name):\n",
    "       df_csv.to_csv(file_name, header='column_names')\n",
    "    else: # else it exists so append without writing the header\n",
    "       df_csv.to_csv(file_name, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_columns = ['model','Loss(Cross_entropy)','Train_Acc', 'Validation_Acc', 'Test_Acc', 'Total_Epoch', 'Total_Duration','TimeStamp']\n",
    "modelaccuracy_file_name = 'train_modelaccuracy.csv'\n",
    "\n",
    "G, labels, train_idx, val_idx, test_idx, num_classes, features = get_acm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(SemanticAttention, self).__init__()\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.project(z).mean(0)                    # (M, 1)\n",
    "        beta = torch.softmax(w, dim=0)                 # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) # (N, M, 1)\n",
    "\n",
    "        return (beta * z).sum(1)                       # (N, D * K)\n",
    "\n",
    "class HANLayer(nn.Module):\n",
    "    def __init__(self, meta_paths, in_size, out_size, layer_num_heads, dropout):\n",
    "        super(HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(len(meta_paths)):\n",
    "            self.gat_layers.append(GATConv(in_size, out_size, layer_num_heads,\n",
    "                                           dropout, dropout, activation=F.elu,\n",
    "                                           allow_zero_in_degree=True))\n",
    "        self.semantic_attention = SemanticAttention(in_size=out_size * layer_num_heads)\n",
    "        self.meta_paths = list(tuple(meta_path) for meta_path in meta_paths)\n",
    "\n",
    "        self._cached_graph = None\n",
    "        self._cached_coalesced_graph = {}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        semantic_embeddings = []\n",
    "\n",
    "        if self._cached_graph is None or self._cached_graph is not g:\n",
    "            self._cached_graph = g\n",
    "            self._cached_coalesced_graph.clear()\n",
    "            for meta_path in self.meta_paths:\n",
    "                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(\n",
    "                        g, meta_path)\n",
    "\n",
    "        for i, meta_path in enumerate(self.meta_paths):\n",
    "            new_g = self._cached_coalesced_graph[meta_path]\n",
    "            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(1))\n",
    "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)                  # (N, M, D * K)\n",
    "\n",
    "        return self.semantic_attention(semantic_embeddings)                            # (N, D * K)\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(HANLayer(meta_paths, in_size, hidden_size, num_heads[0], dropout))\n",
    "        for l in range(1, len(num_heads)):\n",
    "            self.layers.append(HANLayer(meta_paths, hidden_size * num_heads[l-1],\n",
    "                                        hidden_size, num_heads[l], dropout))\n",
    "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gnn in self.layers:\n",
    "            h = gnn(g, h)\n",
    "\n",
    "        return self.predict(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()\n",
    "\n",
    "def score(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    prediction = indices.long().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    accuracy = (prediction == labels).sum() / len(prediction)\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "\n",
    "    return accuracy, micro_f1, macro_f1\n",
    "\n",
    "def evaluate(model, g, features, labels, mask, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "    loss = loss_func(logits[mask], labels[mask])\n",
    "    accuracy, micro_f1, macro_f1 = score(logits[mask], labels[mask]) \n",
    "    return loss, accuracy, micro_f1, macro_f1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_hgcn(total_epoch=10):\n",
    "    print (\"starting full batch HRGCN model\")\n",
    "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "    model = HeterogeneousRGCN(G, 20, 20, 3)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        logits = model(G)\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "        valid_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "        test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if epoch == (total_epoch - 1):\n",
    "            end_time = datetime.datetime.now().replace(microsecond=0)\n",
    "            total_time = end_time - start_time\n",
    "            print (\"Train_Acc :\", train_acc)\n",
    "            print (\"Validation_Acc :\",valid_acc)\n",
    "            print (\"Test_Acc :\",test_acc)\n",
    "            print (\"Total_Epoch :\",total_epoch)\n",
    "            print (\"Total_Duration :\",total_time)\n",
    "            data_accuracy = prepare_accuracy_csv('HeteroRGCN',loss.item(),train_acc.item(), valid_acc.item(), test_acc.item(), total_epoch, total_time, end_time)\n",
    "            write_csv(modelaccuracy_file_name, accuracy_columns, data_accuracy)\n",
    "            print (\"Finished training HRGCN model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "han_configure = {\n",
    "    'lr': 0.005,             # Learning rate\n",
    "    'num_heads': [8],        # Number of attention heads for node-level attention\n",
    "    'hidden_units': 8,\n",
    "    'dropout': 0.6,\n",
    "    'weight_decay': 0.001,\n",
    "    'num_epochs': 200,\n",
    "    'patience': 100\n",
    "}   \n",
    "\n",
    "def run_model_han(num_epochs=10):\n",
    "    model = HAN(meta_paths=[['written-by','writing'],['citing','cited']],\n",
    "                    in_size=features.shape[1],\n",
    "                    hidden_size=han_configure['hidden_units'],\n",
    "                    out_size=num_classes,\n",
    "                    num_heads=han_configure['num_heads'],\n",
    "                    dropout=han_configure['dropout'])\n",
    "    \n",
    "    num_nodes = G.number_of_nodes('paper')\n",
    "    train_mask = get_binary_mask(num_nodes,train_idx)\n",
    "    val_mask = get_binary_mask(num_nodes,val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes,test_idx)\n",
    "    \n",
    "    print('starting HAN model')\n",
    "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=han_configure['lr'], weight_decay=han_configure['weight_decay'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        logits = model(G, features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_acc, train_micro_f1, train_macro_f1 = score(logits[train_mask], labels[train_mask])\n",
    "        val_loss, val_acc, val_micro_f1, val_macro_f1 = evaluate(model, G, features, labels, val_mask, loss_fcn)\n",
    "\n",
    "\n",
    "\n",
    "    test_loss, test_acc, test_micro_f1, test_macro_f1 = evaluate(model, G, features, labels, test_mask, loss_fcn)\n",
    "    \n",
    "    if epoch == (num_epochs - 1):\n",
    "            end_time = datetime.datetime.now().replace(microsecond=0)\n",
    "            total_time = end_time - start_time\n",
    "            print (\"Train_Acc :\", train_acc)\n",
    "            print (\"Validation_Acc :\",val_acc)\n",
    "            print (\"Test_Acc :\",test_acc)\n",
    "            print (\"Total_Epoch :\",num_epochs)\n",
    "            print (\"Total_Duration :\",total_time)\n",
    "            data_accuracy = prepare_accuracy_csv('HAN',loss.item(),train_acc.item(), val_acc.item(), test_acc.item(), num_epochs, total_time, end_time)\n",
    "            write_csv(modelaccuracy_file_name, accuracy_columns, data_accuracy)\n",
    "            print (\"Finished training han model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting HAN model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Acc : 0.9483333333333334\n",
      "Validation_Acc : 0.8933333333333333\n",
      "Test_Acc : 0.8367003367003367\n",
      "Total_Epoch : 100\n",
      "Total_Duration : 0:04:02\n",
      "Finished training han model\n"
     ]
    }
   ],
   "source": [
    "run_model_han(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting full batch HRGCN model\n",
      "Train_Acc : tensor(0.5233)\n",
      "Validation_Acc : tensor(0.4933)\n",
      "Test_Acc : tensor(0.4848)\n",
      "Total_Epoch : 10\n",
      "Total_Duration : 0:00:02\n",
      "Finished training HRGCN model\n"
     ]
    }
   ],
   "source": [
    "run_model_hgcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(model_name,epochs):\n",
    "    if model_name is 'han':\n",
    "        run_model_han(epochs)\n",
    "    elif model_name is 'hgcn':\n",
    "        run_model_hgcn(epochs)\n",
    "    else:\n",
    "        print('Model is not recognized. please use han or hgcn as model name')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting HAN model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n",
      "C:\\anaconda\\envs\\nndlenv\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:25.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Acc : 0.8133333333333334\n",
      "Validation_Acc : 0.8533333333333334\n",
      "Test_Acc : 0.8047138047138047\n",
      "Total_Epoch : 10\n",
      "Total_Duration : 0:00:28\n",
      "Finished training han model\n"
     ]
    }
   ],
   "source": [
    "run_experiments('han',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
