{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'TvsP', 'PvsA', 'PvsV', 'AvsF', 'VvsC', 'PvsL', 'PvsC', 'A', 'C', 'F', 'L', 'P', 'T', 'V', 'PvsT', 'CNormPvsA', 'RNormPvsA', 'CNormPvsC', 'RNormPvsC', 'CNormPvsT', 'RNormPvsT', 'CNormPvsV', 'RNormPvsV', 'CNormVvsC', 'RNormVvsC', 'CNormAvsF', 'RNormAvsF', 'CNormPvsL', 'RNormPvsL', 'stopwords', 'nPvsT', 'nT', 'CNormnPvsT', 'RNormnPvsT', 'nnPvsT', 'nnT', 'CNormnnPvsT', 'RNormnnPvsT', 'PvsP', 'CNormPvsP', 'RNormPvsP']\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "\n",
    "\n",
    "data_file_path = './ACM.mat'\n",
    "data = scipy.io.loadmat(data_file_path)\n",
    "print(list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "still-travel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12499x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12499 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['PvsC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intermediate-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.heterograph({\n",
    "        ('paper', 'written-by', 'author') : data['PvsA'].nonzero(),\n",
    "        ('author', 'writing', 'paper') : data['PvsA'].transpose().nonzero(),\n",
    "        ('paper', 'citing', 'paper') : data['PvsP'].nonzero(),\n",
    "        ('paper', 'cited', 'paper') : data['PvsP'].transpose().nonzero(),\n",
    "        ('paper', 'is-about', 'subject') : data['PvsL'].nonzero(),\n",
    "        ('subject', 'has', 'paper') : data['PvsL'].transpose().nonzero(),\n",
    "        ('paper', 'published-in', 'venue') : data['PvsV'].nonzero(),\n",
    "        ('venue', 'published', 'paper') : data['PvsV'].transpose().nonzero(),\n",
    "        ('paper', 'related-to', 'field') : data['PvsL'].nonzero(),\n",
    "        ('field', 'described-by', 'paper') : data['PvsL'].transpose().nonzero(),\n",
    "        ('paper', 'contains', 'term'):  data['PvsT'].nonzero(),\n",
    "        ('term', 'consist-of', 'paper'):  data['PvsT'].transpose().nonzero()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adjusted-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 17431, 'field': 73, 'paper': 12499, 'subject': 73, 'term': 1903, 'venue': 196},\n",
      "      num_edges={('author', 'writing', 'paper'): 37055, ('field', 'described-by', 'paper'): 12499, ('paper', 'cited', 'paper'): 30789, ('paper', 'citing', 'paper'): 30789, ('paper', 'contains', 'term'): 972973, ('paper', 'is-about', 'subject'): 12499, ('paper', 'published-in', 'venue'): 12499, ('paper', 'related-to', 'field'): 12499, ('paper', 'written-by', 'author'): 37055, ('subject', 'has', 'paper'): 12499, ('term', 'consist-of', 'paper'): 972973, ('venue', 'published', 'paper'): 12499},\n",
      "      metagraph=[('author', 'paper', 'writing'), ('paper', 'paper', 'cited'), ('paper', 'paper', 'citing'), ('paper', 'term', 'contains'), ('paper', 'subject', 'is-about'), ('paper', 'venue', 'published-in'), ('paper', 'field', 'related-to'), ('paper', 'author', 'written-by'), ('field', 'paper', 'described-by'), ('term', 'paper', 'consist-of'), ('subject', 'paper', 'has'), ('venue', 'paper', 'published')])\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "local-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousRGCNLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, etypes):\n",
    "        super(HeterogeneousRGCNLayer, self).__init__()\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(input_size, output_size) for name in etypes\n",
    "            })\n",
    "        \n",
    "        \n",
    "    def forward(self, G, feat_dict):\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "refined-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousRGCN(nn.Module):\n",
    "    def __init__(self, G, input_size, hidden_layer_size, output_size):\n",
    "        super(HeterogeneousRGCN, self).__init__()\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), input_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        self.layer1 = HeterogeneousRGCNLayer(input_size, hidden_layer_size, G.etypes)\n",
    "        self.layer2 = HeterogeneousRGCNLayer(hidden_layer_size, output_size, G.etypes)\n",
    "        \n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        return h_dict['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dutch-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvc = data['PvsC'].tocsr()\n",
    "c_selected = [0, 12, 13]  # KDD, COLT, VLDB\n",
    "p_selected = pvc[:, c_selected].tocoo()\n",
    "\n",
    "labels = pvc.indices\n",
    "labels[labels == 12] = 1\n",
    "labels[labels == 13] = 2\n",
    "labels = torch.tensor(labels).long()\n",
    "\n",
    "# split of train/val/test \n",
    "pid = p_selected.row\n",
    "shuffle = np.random.permutation(pid)\n",
    "train_idx = torch.tensor(shuffle[0:1200]).long()\n",
    "val_idx = torch.tensor(shuffle[1200:1500]).long()\n",
    "test_idx = torch.tensor(shuffle[1500:]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competitive-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_accuracy_csv(loss, train_acc, validation_acc, test_acc, total_epoch, total_duration, timestamp):\n",
    "    accuracy_csv_data = []\n",
    "    accuracy_csv_data.append([str(loss),str(train_acc),str(validation_acc), str(test_acc), str(total_epoch), str(total_duration), str(timestamp)])\n",
    "    return accuracy_csv_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grand-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file_name, columns_name, data):\n",
    "    df_csv = pd.DataFrame(data, columns = columns_name)\n",
    "    if not os.path.isfile(file_name):\n",
    "       df_csv.to_csv(file_name, header='column_names')\n",
    "    else: # else it exists so append without writing the header\n",
    "       df_csv.to_csv(file_name, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "american-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_columns = ['Loss','Train_Acc', 'Validation_Acc', 'Test_Acc', 'Total_Epoch', 'Total_Duration','TimeStamp']\n",
    "modelaccuracy_file_name = 'train_modelaccuracy.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impaired-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(total_epoch):\n",
    "    print (\"Started\")\n",
    "    start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "    model = HeterogeneousRGCN(G, 20, 20, 3)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        logits = model(G)\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "        pred = logits.argmax(1)\n",
    "        train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "        valid_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "        test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if epoch == (total_epoch - 1):\n",
    "            end_time = datetime.datetime.now().replace(microsecond=0)\n",
    "            total_time = end_time - start_time\n",
    "            print (\"Train_Acc :\", train_acc)\n",
    "            print (\"Validation_Acc :\",valid_acc)\n",
    "            print (\"Test_Acc :\",test_acc)\n",
    "            print (\"Total_Epoch :\",total_epoch)\n",
    "            print (\"Total_Duration :\",total_time)\n",
    "            data_accuracy = prepare_accuracy_csv(loss.item(),train_acc.item(), valid_acc.item(), test_acc.item(), total_epoch, total_time, end_time)\n",
    "            write_csv(modelaccuracy_file_name, accuracy_columns, data_accuracy)\n",
    "            print (\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "separate-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "Train_Acc : tensor(1.)\n",
      "Validation_Acc : tensor(0.9567)\n",
      "Test_Acc : tensor(0.9739)\n",
      "Total_Epoch : 100\n",
      "Total_Duration : 0:00:09\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "run_model(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-lender",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
