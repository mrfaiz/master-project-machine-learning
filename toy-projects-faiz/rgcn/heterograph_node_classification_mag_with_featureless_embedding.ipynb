{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dgl.data.utils import load_graphs\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset(name = 'ogbn-mag')\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "def node_level_subsampling(g, list_of_nodes, node_numbers):\n",
    "    subsample_data = {}\n",
    "    if len(list_of_nodes) == 0:\n",
    "        raise Error('list of nodes are empty')\n",
    "    \n",
    "    for node_type in list_of_nodes:\n",
    "        subsample_data[node_type]=g.nodes(node_type)[:node_numbers]\n",
    "    \n",
    "    return dgl.node_subgraph(g,subsample_data)\n",
    "    \n",
    "\n",
    "def create_mask_from_idx(idx,total_nodes):\n",
    "    mask_array = np.zeros(total_nodes,dtype=bool)\n",
    "    mask_array[idx]=True\n",
    "    return mask_array\n",
    "\n",
    "def load_mag_data():\n",
    "    graph = load_graphs('./mag_mp.bin')\n",
    "    _, label = dataset[0]\n",
    "    g = graph[0][0]\n",
    "    print(g)\n",
    "    features = g.ndata['feat']['paper']\n",
    "    labels = label['paper']\n",
    "    mask = torch.BoolTensor(create_mask_from_idx(train_idx['paper'], g.num_nodes('paper')))\n",
    "   \n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Heterograph Conv model\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "class HeteroRGCNLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # W_r for each relation\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}\n",
    "\n",
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, G, in_size, hidden_size, out_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # Use trainable node embeddings as featureless inputs.\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), in_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        # create layers\n",
    "        self.layer1 = HeteroRGCNLayer(in_size, hidden_size, G.etypes)\n",
    "        self.layer2 = HeteroRGCNLayer(hidden_size, out_size, G.etypes)\n",
    "\n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        # get paper logits\n",
    "        return h_dict['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'ai', 'institution'): 1043998, ('author', 'ap', 'paper'): 7145660, ('field_of_study', 'fp', 'paper'): 7505078, ('institution', 'ia', 'author'): 1043998, ('paper', 'pa', 'author'): 7145660, ('paper', 'pf', 'field_of_study'): 7505078, ('paper', 'pp', 'paper'): 5416271},\n",
      "      metagraph=[('author', 'institution', 'ai'), ('author', 'paper', 'ap'), ('institution', 'author', 'ia'), ('paper', 'author', 'pa'), ('paper', 'field_of_study', 'pf'), ('paper', 'paper', 'pp'), ('field_of_study', 'paper', 'fp')])\n",
      "Graph(num_nodes={'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389},\n",
      "      num_edges={('author', 'ai', 'institution'): 1043998, ('author', 'ap', 'paper'): 7145660, ('field_of_study', 'fp', 'paper'): 7505078, ('institution', 'ia', 'author'): 1043998, ('paper', 'pa', 'author'): 7145660, ('paper', 'pf', 'field_of_study'): 7505078, ('paper', 'pp', 'paper'): 5416271},\n",
      "      metagraph=[('author', 'institution', 'ai'), ('author', 'paper', 'ap'), ('institution', 'author', 'ia'), ('paper', 'author', 'pa'), ('paper', 'field_of_study', 'pf'), ('paper', 'paper', 'pp'), ('field_of_study', 'paper', 'fp')])\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, mask = load_mag_data()\n",
    "# g = node_level_subsampling(g,['paper','author','field_of_study','institution'],10000)\n",
    "# g = g.e\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([736389, 128])\n",
      "torch.Size([1134649, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ai', 'ap', 'fp', 'ia', 'pa', 'pf', 'pp']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HeteroRGCN(g, 128, 20, 349)\n",
    "# model = RGCN(len(features), 128, 349, ['cites'])\n",
    "# paper_feats = features[:10000]\n",
    "paper_feats = features\n",
    "author_feats = torch.zeros([g.num_nodes('author'), 128])\n",
    "fos_feats = torch.zeros([g.num_nodes('field_of_study'), 128])\n",
    "institute_feats = torch.zeros([g.num_nodes('institution'), 128])\n",
    "train_mask = mask\n",
    "print(paper_feats.shape)\n",
    "print(author_feats.shape)\n",
    "node_features = {'paper': paper_feats, 'author':author_feats, 'field_of_study':fos_feats, 'institution': institute_feats}\n",
    "# h_dict = model(g, {'paper': paper_feats})\n",
    "g.etypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[246],\n",
       "        [131],\n",
       "        [189],\n",
       "        ...,\n",
       "        [266],\n",
       "        [289],\n",
       "        [  1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model. The output has three logits for three classes.\n",
    "# model = HeteroRGCN(g, 500, 128, 349)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes and extracting the user embeddings\n",
    "    logits = model(g)\n",
    "    loss = F.cross_entropy(logits[train_idx['paper']], labels[train_idx['paper'].squeeze(1)])\n",
    "\n",
    "    pred = logits.argmax(1)\n",
    "#     train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "#     val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "#     test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "#     if best_val_acc < val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_test_acc = test_acc\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "#     if epoch % 5 == 0:\n",
    "#         print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "#             loss.item(),\n",
    "#             train_acc.item(),\n",
    "#             val_acc.item(),\n",
    "#             best_val_acc.item(),\n",
    "#             test_acc.item(),\n",
    "#             best_test_acc.item(),\n",
    "#         ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(loss_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
